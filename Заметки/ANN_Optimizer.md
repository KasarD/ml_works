## Оптимизаторы ИНС
Источник [тут](https://towardsdatascience.com/types-of-optimization-algorithms-used-in-neural-networks-and-ways-to-optimize-gradient-95ae5d39529f)

Оптимизаторы служат для ускорения процесса минимизации/максимизации функции ошибки (Error function). Для каждой модели ИНС можно подобрать такой оптимизатор (из заранее известных), что ее скорость обучения возрастет, как и точность предсказания.
### Типы оптимизаторов
Существующие типы оптимизаторов можно разделить на две группы:
1. *Оптимизирующие алгоритмы первого порядка*. Представляются в виде матрицы частных производных первого порядка (Якобиана). Позволяют увидеть, какой параметр (нейрон) больше всего влияет на ошибку, и изменить его вес на найденную дельту.
2. *Оптимизирующие алгоритмы второго порядка*. Представляются аналогично п. 1, но в матрице представленны производные второго порядка (Хессиан). Это позволяет обратить внимание на кривизну самой функции ошибки.

**Замечание**: алгоритмы второго порядка всегда медленне и прожорливее, нежели первого. С другой стороны, они имеют ряд преимуществ (см [здесь](https://web.stanford.edu/class/msande311/lecture13.pdf)).
В большинстве случаев выгодно выбирать алгоритмы первого порядка, потому что они быстрее сходятся на больших наборах данных.
## Градиентный спуск (SDG) + вариации
Данный алгоритм позволяет использовать механизм обратного распространнеия ошибки для корректировки весов входов каждого из нейронов. Приведем рисунок с формулой функции ошибки и ее графическим представлением
![ф-ия ошибки](https://cdn-images-1.medium.com/max/1200/1*iR7vgbLQ6f70cHHIsSYN2g.png)
Существуют разновидности:
1. **Обычный градиентный спуск**. Вычисляет функцию ошибки только после прохода всего датасета через ИНС. *Медленный и неэффективный*. Гарантирует находжение локального минимума в строго выпуклых функциях.
2. **Стохастический градиентный спуск**. Вычисляет функцию ошибки после прохождения через ИНС каждой из записей набора данных. *Имеет большой разброс (variance)*. Зачастую может просто пролететь наилучший минимум.
θ=θ−η⋅∇J(θ;x(i);y(i)), где x(i), y(i) - значения из датачета, n - скорость обучения.
3. **Mini-batch градиентный спуск**. Взял все лучшее от двух предыдущих. Коррекция ошибки происходит каждый интервал (обычно 10 - 256 записей из датасета). Позволяет избежать "плавания" функции ошибки (как в п.2). **На сегодняшний день является оптимальным**.

Ключевой ловушкой для SDG является Седло, из которого он не сможет выбраться, т.к. градиенты по всем направлениям будут близки к 0. Помимо этого, SDG не стоит применять в тех моделях, параметры которых имеют разную частоту изменчивости. Ведь гр. спуск применяется сразу ко всем параметрам, что может быть плохо при нахождении локальных минимумов.

## Momentum
Вариация SDG, позволяющая избежать ненужного колебания функции ошибки. По сути, данный алгоритм вводит взаимодействие между i-м шагом и i-1 шагом. 
**Формула следующая**:
V(t)=γV(t−1)+η∇J(θ)
θ=θ−V(t)
Коэффициент y обычно устанавливается в 0.9, но может варьироваться для повышения/снижения влияния на i шаг.
## Алгоритм Нестерова (NAG)
Является, в свою очередь, вариацией Momentum, но в более "умной" форме. Если моментум можно представить, как шарик, бездумно скатывающийс явниз по склону в одном направлении, то алгоритм Нестерова предсатвляет собой шар, который знает, что ждет его впереди (плоская поверзность), поэтому он должен затормозить на этой плоской поверхности, чтобы не улететь вверх на следующую горку.
Иными словами, NAG просчитывает движение шара наперед, позволяя предсказывать положение шара в будущем на один шаг.
**Формула следующая**:
V(t)=γV(t−1)+η∇J( θ−γV(t−1) )
θ=θ−V(t)

## Adagrad
Алгоритм, который самостоятельно регулирует коэффициент обучения (η), основываясь на градиенте предыдущего шага.
**Формула следующая**:
![formula](https://cdn-images-1.medium.com/max/1200/1*D4hIuN7AbYqu1yMXZ7AjzQ.png)
Проблема в том, что операнд G является суммой предыдущих градиентов по текущему параметру. Т.к. данный параметр постоянно растет, сам коэф. обучения уменьшается до момента, пока ИНС не перестает обучаться. При этом, сходимость модели замедляется, что требует гораздо больше времени обучения.
## AdaDelta
Является расширением Adagrad, но не накапливает делитель (сумму градиентов), а устанавливает некоторое фиксирование окно суммирования. В общем, данное окно равно среднему всех предыдущих квадратов градиента.
**Формула следующая**:
E[g²] (t)=γ.E[g²] (t−1)+(1−γ).g²(t)
θ(t+1)=θ(t)+Δθ(t).
![](https://cdn-images-1.medium.com/max/1200/1*nuXqz_aJagdYFsWtccRn-g.png)